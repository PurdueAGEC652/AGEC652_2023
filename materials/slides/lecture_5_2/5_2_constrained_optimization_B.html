<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>AGEC 652 - Lecture 5.2</title>
    <meta charset="utf-8" />
    <meta name="author" content="Diego S. Cardoso" />
    <script src="5_2_constrained_optimization_B_files/header-attrs-2.19/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# AGEC 652 - Lecture 5.2
]
.subtitle[
## Constrained optimization
</p>
Part B: Using Julia
]
.author[
### Diego S. Cardoso
]
.date[
### Spring 2023
]

---

exclude: true

```r
if (!require("pacman")) install.packages("pacman")
```

```
## Loading required package: pacman
```

```r
pacman::p_load(
  xaringanthemer, JuliaCall
)

#options(htmltools.dir.version = FALSE)

knitr::opts_hooks$set(fig.callout = function(options) {
  if (options$fig.callout) {
    options$echo &lt;- FALSE
  }

  knitr::opts_chunk$set(echo = TRUE, fig.align="center")
  options
})
```




```julia
using Pkg
Pkg.activate(".")
Pkg.instantiate()
#Pkg.add("JuMP")
#Pkg.add("Ipopt")
```


---

## .blue[Course roadmap]

1. .gray[Intro to Scientific Computing]
2. .gray[Numerical operations and representations]
3. .gray[Systems of equations]
4. .gray[Function approximation (Skipped)]
5. **Optimization**
  - .gold[5.1 Unconstrained optimization]
  - **5.2 Constrained optimization**
    - A) **Theory and solution algorithms** `\(\leftarrow\)` *You are here*
    - B) Constrained optimization in Julia
6. Structural estimation


---

class: inverse, center, middle

.footnote[\*These slides are based on Miranda &amp; Fackler (2002), Nocedal &amp; Wright (2006), Judd (1998), and course materials by Ivan Rudik and Florian Oswald.]

---

class: inverse, center, middle

# Constrained optimization in Julia

---

## Constrained optimization in Julia


&lt;div style="float:right"&gt;
  &lt;img src="figures/van_halen_jump.jpg" height=400&gt;
&lt;/div&gt;

We are going to cover a cool package called `JuMP.jl`
- It offers a whole modeling language inside Julia
- You define your model and plug it into one of the [many solvers available](https://jump.dev/JuMP.jl/stable/installation/#Supported-solvers)
- It's like GAMS and AMPL... *but FREE and with a full-fledged programming language around it*


---

## Constrained optimization in Julia

Most solvers can be accessed directly in their own packages
- Like we did to use `Optim.jl`
- These packages are usually just a Julia interface for a solver programmed in another language

--

But `JuMP` gives us a unified way of specifying our models and switching between solvers

`JuMP` specifically designed for constrained optimization but works with unconstrained too 
- With more overhead relative to using `Optim` or `NLopt` directly

---

## Getting stated with JuMP

There are 5 key steps:

1) Initialize your model and solver: 
- `mymodel = Model(SomeOptimizer)`

--

2) Declare variables (adding any box constraints)
- `@variable(mymodel, x &gt;= 0)`

--

3) Declare the objective function

- If linear: `@objective(mymodel, Min, 12x + 20y)`
- If nonlinear: `@NLobjective(mymodel, Min, 12x^0.7 + 20y^2)`

---

## Getting stated with JuMP

4) Declare constraints
- If linear: `@constraint(mymodel, c1, 6x + 8y &gt;= 100)`
- If nonlinear: `@NLconstraint(mymodel, c1, 6x^2 - 2y &gt;= 100)`

--

5) Solve it
- `optimize!(mymodel)`
- Note the `!`, so we are modifying `mymodel` and saving results in this object

---

## .blue[Follow along!]

Let's use `JuMP` to solve the illustrative problem from the first slides

We will use solver `Ipopt`, which stands for *Interior Point Optimizer*. It's a free solver we can access through package `Ipopt.jl`


```julia
using JuMP, Ipopt;
```

---

## .blue[Follow along: function definition]

Define the function:
$$
\min_x -exp\left(-(x_1 x_2 - 1.5)^2 - (x_2 - 1.5)^2 \right)
$$


```julia
f(x1,x2) = -exp.(-(x1.*x2 - 3/2).^2 - (x2-3/2).^2);
```

---

## .blue[Follow along: initialize model]


Initialize the model for `Ipopt`

```julia
model = Model(Ipopt.Optimizer)
```

```
## A JuMP Model
## Feasibility problem with:
## Variables: 0
## Model mode: AUTOMATIC
## CachingOptimizer state: EMPTY_OPTIMIZER
## Solver name: Ipopt
```

--

You can set optimzer parameters like this
- There are TONS of parameters you can adjust (see the [manual](https://coin-or.github.io/Ipopt/OPTIONS.html))


```julia
# This is relative tol. Default is 1e-8
set_optimizer_attribute(model, "tol", 1e-6) 
```


---

## .blue[Follow along: declare variables]

We will focus on non-negative values 


```julia
@variable(model, x1 &gt;=0)
```

```
## x1
```

```julia
@variable(model, x2 &gt;=0)
```

```
## x2
```

- You could type `@variable(model, x1)` to declare a `\(x_1\)` as a free variable


---

## .blue[Follow along: declare objective]

We will focus on non-negative values 


```julia
@NLobjective(model, Min, f(x1, x2))
```

--

`JuMP` will use autodiff (with `ForwardDiff` package) by default. If you want to use your define gradient and Hessian, you need to "register" the function like this
```
register(model, :my_f, n, f, grad, hessian)
```
- `:my_f` is the name you want to use inside `model`, `n` is the number of variables `f` takes, and `grad` `hessian` are user-defined functions


---

## .blue[Follow along: solving the model]

First, let's solve the (mostly) unconstrained problem
- Not really unconstrained because we defined non-negative `x1` and `x2` 

Checking our model

```julia
print(model)
```

```
## Min f(x1, x2)
## Subject to
##  x1 &gt;= 0.0
##  x2 &gt;= 0.0
```

---

## .blue[Follow along: solving the model]


```julia
optimize!(model)
```

```
## 
## ******************************************************************************
## This program contains Ipopt, a library for large-scale nonlinear optimization.
##  Ipopt is released as open source code under the Eclipse Public License (EPL).
##          For more information visit https://github.com/coin-or/Ipopt
## ******************************************************************************
## 
## This is Ipopt version 3.14.4, running with linear solver MUMPS 5.5.1.
## 
## Number of nonzeros in equality constraint Jacobian...:        0
## Number of nonzeros in inequality constraint Jacobian.:        0
## Number of nonzeros in Lagrangian Hessian.............:        0
## 
## Total number of variables............................:        2
##                      variables with only lower bounds:        2
##                 variables with lower and upper bounds:        0
##                      variables with only upper bounds:        0
## Total number of equality constraints.................:        0
## Total number of inequality constraints...............:        0
##         inequality constraints with only lower bounds:        0
##    inequality constraints with lower and upper bounds:        0
##         inequality constraints with only upper bounds:        0
## 
## iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls
##    0 -1.1449605e-02 0.00e+00 1.03e+00   0.0 0.00e+00    -  0.00e+00 0.00e+00   0
##    1 -1.3138288e-02 0.00e+00 5.01e-02  -1.4 4.40e-02    -  1.00e+00 1.00e+00f  1
##    2 -1.4859511e-02 0.00e+00 4.47e-02  -3.4 4.04e-02    -  1.00e+00 1.00e+00f  1
##    3 -1.6966706e-02 0.00e+00 4.95e-02  -4.8 4.45e-02    -  1.00e+00 1.00e+00f  1
##    4 -1.9620344e-02 0.00e+00 5.56e-02  -5.5 4.94e-02    -  1.00e+00 1.00e+00f  1
##    5 -2.3077725e-02 0.00e+00 6.35e-02  -6.3 5.56e-02    -  1.00e+00 1.00e+00f  1
##    6 -2.7794373e-02 0.00e+00 7.43e-02  -6.8 6.35e-02    -  1.00e+00 1.00e+00f  1
##    7 -3.4670094e-02 0.00e+00 9.00e-02  -7.3 7.43e-02    -  1.00e+00 1.00e+00f  1
##    8 -4.5742703e-02 0.00e+00 1.15e-01  -7.8 9.00e-02    -  1.00e+00 1.00e+00f  1
##    9 -6.6634419e-02 0.00e+00 1.62e-01  -8.1 1.15e-01    -  1.00e+00 1.00e+00f  1
## iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls
##   10 -1.1847930e-01 0.00e+00 2.76e-01  -8.6 1.62e-01    -  1.00e+00 1.00e+00f  1
##   11 -3.3377241e-01 0.00e+00 6.61e-01  -8.5 2.76e-01    -  1.00e+00 1.00e+00f  1
##   12 -7.4524828e-01 0.00e+00 1.28e+00  -8.5 6.61e-01    -  1.00e+00 1.00e+00f  1
##   13 -9.4053727e-01 0.00e+00 6.18e-01 -11.0 1.28e+00    -  1.00e+00 2.30e-01f  3
##   14 -9.9989242e-01 0.00e+00 3.10e-02  -9.6 9.60e-02    -  1.00e+00 1.00e+00f  1
##   15 -9.9998421e-01 0.00e+00 8.53e-03 -11.0 5.07e-03    -  1.00e+00 1.00e+00f  1
##   16 -9.9999077e-01 0.00e+00 4.17e-03 -11.0 1.04e-03    -  1.00e+00 1.00e+00f  1
##   17 -9.9999761e-01 0.00e+00 3.13e-03 -11.0 1.78e-03    -  1.00e+00 1.00e+00f  1
##   18 -9.9999999e-01 0.00e+00 2.95e-04 -11.0 1.38e-03    -  1.00e+00 1.00e+00f  1
##   19 -1.0000000e+00 0.00e+00 3.32e-06 -11.0 7.45e-05    -  1.00e+00 1.00e+00f  1
## iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls
##   20 -1.0000000e+00 0.00e+00 1.33e-07 -11.0 5.74e-07    -  1.00e+00 1.00e+00f  1
## 
## Number of Iterations....: 20
## 
##                                    (scaled)                 (unscaled)
## Objective...............:  -9.9999999999999656e-01   -9.9999999999999656e-01
## Dual infeasibility......:   1.3262150989499029e-07    1.3262150989499029e-07
## Constraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00
## Variable bound violation:   0.0000000000000000e+00    0.0000000000000000e+00
## Complementarity.........:   9.9999999999991302e-12    9.9999999999991302e-12
## Overall NLP error.......:   1.3262150989499029e-07    1.3262150989499029e-07
## 
## 
## Number of objective function evaluations             = 27
## Number of objective gradient evaluations             = 21
## Number of equality constraint evaluations            = 0
## Number of inequality constraint evaluations          = 0
## Number of equality constraint Jacobian evaluations   = 0
## Number of inequality constraint Jacobian evaluations = 0
## Number of Lagrangian Hessian evaluations             = 0
## Total seconds in IPOPT                               = 2.755
## 
## EXIT: Optimal Solution Found.
```

---

## .blue[Follow along: solving the model]

The return message is rather long and contains many details about the execution. You can turn this message off with


```julia
set_silent(model);
```

We can check minimizers with




```julia
unc_x1 = value(x1)
```

```
## 1.0000000326687912
```

```julia
unc_x2 = value(x2)
```

```
## 1.4999999423446968
```

```julia
unc_obj = objective_value(model)
```

```
## -0.9999999999999966
```

---

## .blue[Follow along: solving the model]


And the minimum with


```julia
unc_obj = objective_value(model)
```

```
## -0.9999999999999966
```


---

## .blue[Follow along: solving the model]

&lt;div align="center"&gt;
  &lt;img src="figures/jump_ex1.png", height=450&gt;
&lt;/div&gt;


---

## .blue[Follow along: declaring constraints]

Let's add a nonlinear equality constraint `\(-x_1 + x^2 = 0\)` and re-solve the model

```julia
@NLconstraint(model, -x1 +x2^2 == 0)
```

```
## (-x1 + x2 ^ 2.0) - 0.0 == 0
```

```julia
print(model)
```

```
## Min f(x1, x2)
## Subject to
##  x1 &gt;= 0.0
##  x2 &gt;= 0.0
##  (-x1 + x2 ^ 2.0) - 0.0 == 0
```

```julia
optimize!(model)
```

---

## .blue[Follow along: solving the equality constrained model]




```julia
eqcon_x1 = value(x1)
```

```
## 1.3578043097074932
```

```julia
eqcon_x2 = value(x2)
```

```
## 1.1652486042512478
```

```julia
value(-x1 + x2^2) # We can evaluate expressions too
```

```
## 1.9877433032888803e-12
```

```julia
eqcon_obj = objective_value(model)
```

```
## -0.887974742266783
```

---

## .blue[Follow along: solving the equality constrained model]

&lt;div align="center"&gt;
  &lt;img src="figures/jump_ex2.png", height=450&gt;
&lt;/div&gt;

---

## Solving the inequality constrained model

I now initialize a new model with inequality constraint `\(-x_1 + x^2 \leq 0\)`


```julia
model2 = Model(Ipopt.Optimizer);
@variable(model2, x1 &gt;=0);
@variable(model2, x2 &gt;=0);
@NLobjective(model2, Min, f(x1, x2));
@NLconstraint(model2, -x1 + x2^2 &lt;= 0);
optimize!(model2);
```

---

## Solving the inequality constrained model


```julia
ineqcon_x1 = value(x1)
```

```
## 1.357804311747407
```

```julia
ineqcon_x2 = value(x2)
```

```
## 1.165248609391406
```

```julia
ineqcon_obj = objective_value(model2)
```

```
## -0.887974743957088
```

Same results as in the equality constraint: the constraint is binding

---

## Solving the inequality constrained model

&lt;div align="center"&gt;
  &lt;img src="figures/jump_ex3.png", height=450&gt;
&lt;/div&gt;

---

## Relaxing the inequality constraint

What if instead we use inequality constraint `\(-x_1 + x^2 \leq 1.5\)`?


```julia
model3 = Model(Ipopt.Optimizer);
@variable(model3, x1 &gt;=0);
@variable(model3, x2 &gt;=0);
@NLobjective(model3, Min, f(x1, x2));
@NLconstraint(model3, c1, -x1 + x2^2 &lt;= 1.5);
optimize!(model3);
```

---

## Relaxing the inequality constraint


```julia
ineqcon2_obj = objective_value(model3)
```

```
## -1.0
```

```julia
ineqcon2_x1 = value(x1)
```

```
## 1.0000000035503052
```

```julia
ineqcon2_x2 = value(x2)
```

```
## 1.4999999931258383
```

We get the same results as in the unconstrained case

---

## Solving the inequality constrained model

&lt;div align="center"&gt;
  &lt;img src="figures/jump_ex4.png", height=450&gt;
&lt;/div&gt;


---

class: inverse, center, middle

# Practical advice for numerical optimization

---


## Best practices for optimization

Plug in your guess, let the solver go, and you're done right?

--

## WRONG!

--

These algorithms are not guaranteed to always find even a local solution,
you need to test and make sure you are converging correctly

---

## Check return codes

Return codes (or exit flags) tell you why the solver stopped
- There are all sorts of reasons why a solver ends execution
- Each solver has its own way of reporting errors
- In `JuMP` you can use `@show termination_status(mymodel)`

**READ THE SOLVER DOCUMENTATION!**

--

Use trace options to get a sense of what went wrong
- Did guesses grow unexpectedly?
- Did a gradient-based operation fail? (E.g., division by zero)

---

## Check return codes

Examples from [Ipopt.jl documentation](https://coin-or.github.io/Ipopt/OUTPUT.html)

&lt;div align="center"&gt;
  &lt;img src="figures/ipopt_return_codes.png" height=550&gt;
&lt;/div&gt;

---

## Try alternative algorithms

Optimization is approximately 53% art

--

Not all algorithms are suited for every problem `\(\rightarrow\)` it is useful to check how different algorithms perform

--

Interior-point is usually the default in constrained optimization solvers (low memory usage, fast), but try other algorithms and see if the solution generally remains the same

---

## Problem scaling

The **scaling** of a problem matters for optimization performance

--

A problem is **poorly scaled** if changes to `\(x\)` in a certain direction
produce much bigger changes in `\(f\)` than changes to in `\(x\)` in another direction

--

&lt;div align="center"&gt;
  &lt;img src="figures/poorly_scaled.png" height=400&gt;
&lt;/div&gt;

---

## Problem scaling

Ex: `\(f(x) = 10^9 x_1^2 + x_2^2\)` is poorly scaled

--

This happens when things change at different rates:
- Investment rates are between 0 and 1
- Consumption can be in trillions of dollars

--

How do we solve this issue?

--

Rescale the problem: put them in units that are generally within an order of magnitude of 1
- Investment rate in percentage terms: `\(0\%-100\%\)`
- Consumption in units of trillion dollars instead of dollars

---

## Be aware of tolerances

Two main tolerances in optimization:

1. `ftol` is the tolerance for the change in the function value (absolute and relative)
2. `xtol` is the tolerance for the change in the input values (absolute and relative)

--

What is a suitable tolerance?


---

## Be aware of tolerances

It depends

--

Explore sensitivity to tolerance, typically pick a conservative (small) number
- Defaults in solvers are usually `1e-6`

If you are using simulation-based estimators or estimators that depend on successive optimizations, be even more conservative *because errors compound*

---

## Be aware of tolerances

May be a substantial trade-off between accuracy of your solution and speed

--

Common bad practice is to pick a larger tolerance (e.g. `1e-3`) so the problem "works" (e.g. so your big MLE converges)

--

Issue is that `1e-3` might be pretty big for your problem if you haven't checked that your solution is not sensitive to the tolerance

---

## Perturb your initial guesses

**Initial guesses matter**

--

Good ones can improve performance
- E.g. initial guess for next iteration of coefficient estimates should be current iteration estimates

--

Bad ones can give you terrible performance, or wrong answers if your problem isn't perfect
- E.g. bad scaling, not well-conditioned, multiple equilibria






    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
